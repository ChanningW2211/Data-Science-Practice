---
title: "Model Selection"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import
(@) Produce a tidier file automobile.csv
```{r}
o <- read.csv("automobile-original.csv", na.strings = "?")
o <- na.omit(o)
o <- subset(o, select = -c(engine.location))
write.csv(o, "automobile.csv", row.names = FALSE)

d1 <- read.csv("automobile.csv")
d2 <- read.csv("automobile-subset.csv")
all(d1 == d2)
```

# Explore
(@) The mean price (price) of all vehicles
```{r}
mean(d1$price)
```

The number of vehicles that have 4 doors (num.of.doors)
```{r}
nrow(d1[d1$num.of.doors == "four",])
```

The different engine types (engine.type)
```{r}
unique(d1$engine.type)
```

The number of vehicles that have a price (price) higher than $20000
```{r}
nrow(d1[d1$price > 20000,])
```

The mean price (price) for “4wd” (drive.wheels)
```{r}
mean(d1[d1$drive.wheels == "4wd",]$price)
```

(@) Produce pairwise scatterplots between variables normalized.losses, wheel.base, peak.rpm and price
```{r}
pairs(~normalized.losses + wheel.base + peak.rpm +price, data=d1)
```

# Linear regression
For all of the following regression questions, we use the price (price) as the response variable.

(@) Produce the full linear regression model with all variables included. Comment on the outcome.
```{r}
fit <- lm(price~., d1)
summary(fit)
```
There are too many variables in this linear model, where some of variables are significant but could be highly correlated. Also, 3 coeffiences are not available.

(@) Remove any variable(s) that seem to cause the linear regression to fail, i.e., some coefficients may become NA. Repeat this until you can produce a meaningful “full” linear regression model (it is okay if you remove slightly more variables than necessary).

The number of variables are deemed to be significant by the t-tests (with a p-value less than 0.05)
```{r}
fit <- lm(price ~ . - fuel.system - num.of.cylinders - engine.type, d1)
coef <- summary(fit)$coefficients
table(coef[,4]<0.05)
```
There're 24 significant variables.

(@) Apply the “full” linear regression model to the data and compute the resulting mean squared error (MSE).
```{r}
mse = function(y1, y2) mean( (y1 - y2)^2 )
mse(predict(fit, d1), d1$price)
```

# Subset selection
```{r}
library(leaps)
library(glmnet)
```

(@) For the data (using your “full” set of variables), produce a subset linear regression model, using the backward selection and the AIC.
```{r}
r.bwd = regsubsets(price ~ . - fuel.system - num.of.cylinders - engine.type, nvmax = 38, data=d1, method="backward")
r.s <- summary(r.bwd)
bic <- r.s$bic
aic <- bic - (log(nrow(d1)) - 2) * (38:1)
j = which.min(aic)
beta = coef(r.bwd, j)
beta
```

(@) Apply the AIC-selected model to the data and compute the resulting MSE.
```{r}
d1.matrix <- model.matrix(fit)
yhat = drop(d1.matrix[,names(beta)] %*% beta)
mse(yhat, d1$price)
```

(@) Create a plot that shows the predictions of your AIC-selected model against the response variable (price), using different colors for different levels of drive.wheels

# LASSO
(@) For the data (using your “full” set of variables), compute the Lasso model.
```{r}
x <- d1.matrix[,-1]
y <- d1$price
r.lasso = glmnet(x, y, alpha=1)
```

(@) Create a coefficient profile plot of the coefficient paths that varies with the value of λ (or log(λ)).
```{r}
plot(r.lasso, main="Coefficient Profile")
```
(@) Choose 5 different λ-values within a seemingly reasonable range (with roughly 5 to 30 variables included) and compute the MSEs of the corresponding 5 Lasso subset models. Write R code to find out how many variables (excluding the intercept) are included in each Lasso subset model.

# Summary
In this lab, we went though the data science process with a focus on the model with lots of variable. We used two main techniques to choose the best submodel, model selection criteria and regularisation.

There're 2 main criteria, namely, AIC and BIC, they based the maximum likelihood with the only different on the penalty terms. AIC tends to preserve the variables while BIC penalise heavily on the numbers of variables with the growth of observations.

The regularisation approaches shrink the coefficients instead. To that, Lasso can do it more efficient by shrinking the coefficients to exactly 0.
















