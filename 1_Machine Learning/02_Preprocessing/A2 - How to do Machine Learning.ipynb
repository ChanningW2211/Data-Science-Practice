{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ReliefF import ReliefF\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from autorank import autorank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "# read data\n",
    "data = pd.read_csv('data_A2.csv')\n",
    "labels = pd.read_csv('labels_A2.csv')\n",
    "# Fill the missing value with -1\n",
    "X_fill = np.nan_to_num(data, copy=True, nan=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the missing values scatter among different tuples, so we can't simply delete them. And data itself covers a range of different values so we rule out imputation and instead fill in a global constant to not undermine the variabiliy while remain validilty of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#set a random state\n",
    "RANDOM_STATE = 1234\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "#pick the 10 most important features using decision tree\n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "dt.fit(X_fill, labels)\n",
    "fi = dt.feature_importances_\n",
    "X_clean = X_fill[:,np.argsort(fi)[:10]]\n",
    "# fs = ReliefF(n_neighbors=10, n_features_to_keep=10)\n",
    "# X_clean = fs.fit_transform(X_fill, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found them by using the fitted decision tree classifier to calculate the gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                      meanrank    median         mad  ci_lower  ci_upper  \\\n",
      "RandomForest             1.080  0.633503   0.0271872  0.620957  0.646789   \n",
      "UnprunedDecisionTree     2.080  0.577858   0.0277299  0.568584  0.590106   \n",
      "DecisionStump            2.895  0.531812   0.0230604  0.526027  0.551136   \n",
      "PrunedDecisionTree       3.945  0.494960  0.00622718  0.490909  0.498403   \n",
      "\n",
      "                     effect_size   magnitude  \n",
      "RandomForest                   0  negligible  \n",
      "UnprunedDecisionTree     2.02644       large  \n",
      "DecisionStump            4.03403       large  \n",
      "PrunedDecisionTree       7.02479       large  \n",
      "pvalue=1.1964609018114955e-57\n",
      "cd=0.46903593329832804\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[3.85248233314428e-09, 2.6066192225131853e-16, 4.710492618187345e-08, 4.803963877495236e-15]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.0648672740107634\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "df = pd.DataFrame(columns=['RandomForest', 'PrunedDecisionTree', 'UnprunedDecisionTree', 'DecisionStump'])\n",
    "for x in np.random.rand(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, labels.values.ravel(), test_size=x, random_state=RANDOM_STATE)\n",
    "\n",
    "    rf = RandomForestClassifier(max_depth=10, random_state= RANDOM_STATE)\n",
    "    rf.fit(X_train,y_train)\n",
    "\n",
    "    dt1 = DecisionTreeClassifier(random_state=RANDOM_STATE, ccp_alpha= 0.2)\n",
    "    dt1.fit(X_train, y_train)\n",
    "\n",
    "    dt2 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    dt2.fit(X_train, y_train)\n",
    "\n",
    "    dt3 = DecisionTreeClassifier(random_state=RANDOM_STATE,max_depth=1)\n",
    "    dt3.fit(X_train, y_train)\n",
    "    \n",
    "    df = df.append({'RandomForest':rf.score(X_test, y_test), 'PrunedDecisionTree':dt1.score(X_test, y_test), 'UnprunedDecisionTree':dt2.score(X_test, y_test), 'DecisionStump': dt3.score(X_test, y_test)},ignore_index=True)\n",
    "\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prunned decision tree performed the worst as it's overfitting the randomised data and random forest performs the best as it combines features which are not correlated to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                      meanrank    median         mad  ci_lower  ci_upper  \\\n",
      "RandomForest             1.045  0.627426   0.0312963  0.617347  0.647702   \n",
      "UnprunedDecisionTree     2.135  0.574799   0.0305722  0.561315  0.591241   \n",
      "DecisionStump            2.865  0.541329   0.0198218   0.53405  0.554415   \n",
      "PrunedDecisionTree       3.955  0.495376  0.00453343  0.492674  0.497449   \n",
      "\n",
      "                     effect_size   magnitude  \n",
      "RandomForest                   0  negligible  \n",
      "UnprunedDecisionTree     1.70113       large  \n",
      "DecisionStump            3.28675       large  \n",
      "PrunedDecisionTree       5.90542       large  \n",
      "pvalue=4.50247885017667e-59\n",
      "cd=0.46903593329832804\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.013476056046783924, 1.996535404272701e-16, 9.369413231374857e-12, 3.637159534264356e-05]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=1.1296501742177784e-05\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "# additive normal noise\n",
    "noise = np.random.normal(0, 0.2, np.shape(X_clean))\n",
    "X_noise = X_clean + np.multiply(noise, np.average(X_train, axis=0))\n",
    "\n",
    "df = pd.DataFrame(columns=['RandomForest', 'PrunedDecisionTree', 'UnprunedDecisionTree', 'DecisionStump'])\n",
    "for x in np.random.rand(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_noise, labels.values.ravel(), test_size=x, random_state=RANDOM_STATE)\n",
    "\n",
    "    rf = RandomForestClassifier(max_depth=10, random_state= RANDOM_STATE)\n",
    "    rf.fit(X_train,y_train)\n",
    "\n",
    "    dt1 = DecisionTreeClassifier(random_state=RANDOM_STATE, ccp_alpha= 0.2)\n",
    "    dt1.fit(X_train, y_train)\n",
    "\n",
    "    dt2 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    dt2.fit(X_train, y_train)\n",
    "\n",
    "    dt3 = DecisionTreeClassifier(random_state=RANDOM_STATE,max_depth=1)\n",
    "    dt3.fit(X_train, y_train)\n",
    "    \n",
    "    df = df.append({'RandomForest':rf.score(X_test, y_test), 'PrunedDecisionTree':dt1.score(X_test, y_test), 'UnprunedDecisionTree':dt2.score(X_test, y_test), 'DecisionStump': dt3.score(X_test, y_test)},ignore_index=True)\n",
    "\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest performs signifantly better as the noise can be corrected by combination of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                      meanrank    median         mad  ci_lower  ci_upper  \\\n",
      "DecisionStump            2.275  0.497842   0.0163724  0.477833  0.506472   \n",
      "UnprunedDecisionTree     2.440  0.489858   0.0272635  0.477509  0.503145   \n",
      "RandomForest             2.625  0.488859   0.0195547  0.477912       0.5   \n",
      "PrunedDecisionTree       2.660  0.494109  0.00583907  0.483553  0.496454   \n",
      "\n",
      "                     effect_size   magnitude  \n",
      "DecisionStump                  0  negligible  \n",
      "UnprunedDecisionTree    0.355036       small  \n",
      "RandomForest            0.498115       small  \n",
      "PrunedDecisionTree      0.303658       small  \n",
      "pvalue=0.11760300259309563\n",
      "cd=0.46903593329832804\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[5.187140232010279e-06, 3.3028347771353053e-16, 2.813774472087971e-06, 3.8545308302584055e-14]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.33494209462884944\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "# multiplicative normal noise\n",
    "noise = np.random.normal(0, 0.2, np.shape(X_clean))\n",
    "X_noise = np.multiply(X_clean, noise)\n",
    "\n",
    "df = pd.DataFrame(columns=['RandomForest', 'PrunedDecisionTree', 'UnprunedDecisionTree', 'DecisionStump'])\n",
    "for x in np.random.rand(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_noise, labels.values.ravel(), test_size=x, random_state=RANDOM_STATE)\n",
    "\n",
    "    rf = RandomForestClassifier(max_depth=10, random_state= RANDOM_STATE)\n",
    "    rf.fit(X_train,y_train)\n",
    "\n",
    "    dt1 = DecisionTreeClassifier(random_state=RANDOM_STATE, ccp_alpha= 0.2)\n",
    "    dt1.fit(X_train, y_train)\n",
    "\n",
    "    dt2 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    dt2.fit(X_train, y_train)\n",
    "\n",
    "    dt3 = DecisionTreeClassifier(random_state=RANDOM_STATE,max_depth=1)\n",
    "    dt3.fit(X_train, y_train)\n",
    "    \n",
    "    df = df.append({'RandomForest':rf.score(X_test, y_test), 'PrunedDecisionTree':dt1.score(X_test, y_test), 'UnprunedDecisionTree':dt2.score(X_test, y_test), 'DecisionStump': dt3.score(X_test, y_test)},ignore_index=True)\n",
    "\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classifiers are worsen in this case as the noise can't be corrected due to randomised scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/channingwang/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/morestats.py:1678: UserWarning: Input data for shapiro has range zero. The results may not be accurate.\n",
      "  warnings.warn(\"Input data for shapiro has range zero. The results \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                      meanrank  median       mad ci_lower ci_upper  \\\n",
      "RandomForest             1.535   0.550  0.044478     0.53     0.57   \n",
      "UnprunedDecisionTree     2.075   0.515  0.051891     0.49     0.54   \n",
      "DecisionStump            2.460   0.480  0.088956     0.43     0.55   \n",
      "PrunedDecisionTree       3.930   0.400         0      0.4      0.4   \n",
      "\n",
      "                     effect_size   magnitude  \n",
      "RandomForest                   0  negligible  \n",
      "UnprunedDecisionTree    0.724235      medium  \n",
      "DecisionStump           0.995366       large  \n",
      "PrunedDecisionTree       4.76937       large  \n",
      "pvalue=1.260484602570955e-41\n",
      "cd=0.46903593329832804\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.05064091831445694, 1.0, 0.33107009530067444, 2.1045818243692338e-07]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=2.789510644309798e-47\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n",
      "RankResult(rankdf=\n",
      "                      meanrank  median       mad ci_lower ci_upper  \\\n",
      "RandomForest             1.440    0.48  0.059304     0.46     0.51   \n",
      "UnprunedDecisionTree     1.815    0.45  0.044478     0.43     0.48   \n",
      "DecisionStump            3.230    0.40         0      0.4     0.41   \n",
      "PrunedDecisionTree       3.515    0.40         0      0.4      0.4   \n",
      "\n",
      "                     effect_size   magnitude  \n",
      "RandomForest                   0  negligible  \n",
      "UnprunedDecisionTree    0.572324      medium  \n",
      "DecisionStump            1.90775       large  \n",
      "PrunedDecisionTree       1.90775       large  \n",
      "pvalue=3.2470568055732433e-44\n",
      "cd=0.46903593329832804\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.035578273236751556, 1.0, 0.031193194910883904, 2.212469617235424e-11]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=4.6189838773291195e-50\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/channingwang/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/morestats.py:1678: UserWarning: Input data for shapiro has range zero. The results may not be accurate.\n",
      "  warnings.warn(\"Input data for shapiro has range zero. The results \"\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "#Adding noise to trainning set\n",
    "df = pd.DataFrame(columns=['RandomForest', 'PrunedDecisionTree', 'UnprunedDecisionTree', 'DecisionStump'])\n",
    "for x in np.random.rand(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, labels.values.ravel(), test_size=100, random_state=RANDOM_STATE)\n",
    "\n",
    "    noise = np.random.normal(0, 0.2, np.shape(X_train))\n",
    "    X_train = np.multiply(X_train, noise)\n",
    "    \n",
    "    rf = RandomForestClassifier(max_depth=10, random_state= RANDOM_STATE)\n",
    "    rf.fit(X_train,y_train)\n",
    "\n",
    "    dt1 = DecisionTreeClassifier(random_state=RANDOM_STATE, ccp_alpha= 0.2)\n",
    "    dt1.fit(X_train, y_train)\n",
    "\n",
    "    dt2 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    dt2.fit(X_train, y_train)\n",
    "\n",
    "    dt3 = DecisionTreeClassifier(random_state=RANDOM_STATE,max_depth=1)\n",
    "    dt3.fit(X_train, y_train)\n",
    "    \n",
    "    df = df.append({'RandomForest':rf.score(X_test, y_test), 'PrunedDecisionTree':dt1.score(X_test, y_test), 'UnprunedDecisionTree':dt2.score(X_test, y_test), 'DecisionStump': dt3.score(X_test, y_test)},ignore_index=True)\n",
    "\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)\n",
    "\n",
    "#Adding noise to test set\n",
    "df = pd.DataFrame(columns=['RandomForest', 'PrunedDecisionTree', 'UnprunedDecisionTree', 'DecisionStump'])\n",
    "for x in np.random.rand(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, labels.values.ravel(), test_size=100, random_state=RANDOM_STATE)\n",
    "\n",
    "    noise = np.random.normal(0, 0.2, np.shape(X_test))\n",
    "    X_test = np.multiply(X_test, noise)\n",
    "    \n",
    "    rf = RandomForestClassifier(max_depth=10, random_state= RANDOM_STATE)\n",
    "    rf.fit(X_train,y_train)\n",
    "\n",
    "    dt1 = DecisionTreeClassifier(random_state=RANDOM_STATE, ccp_alpha= 0.2)\n",
    "    dt1.fit(X_train, y_train)\n",
    "\n",
    "    dt2 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    dt2.fit(X_train, y_train)\n",
    "\n",
    "    dt3 = DecisionTreeClassifier(random_state=RANDOM_STATE,max_depth=1)\n",
    "    dt3.fit(X_train, y_train)\n",
    "    \n",
    "    df = df.append({'RandomForest':rf.score(X_test, y_test), 'PrunedDecisionTree':dt1.score(X_test, y_test), 'UnprunedDecisionTree':dt2.score(X_test, y_test), 'DecisionStump': dt3.score(X_test, y_test)},ignore_index=True)\n",
    "\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the noise into the test set has a bigger impact as we base on a wrong set to train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
