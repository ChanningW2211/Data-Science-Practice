{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ReliefF import ReliefF\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from autorank import autorank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the missing values scatter across the data set, so we can't simply delete them. And data itself covers a range of different values so we rule out imputation and instead fill in a global constant not to undermine the variabiliy while remain validility of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "X = pd.read_csv('data_A2.csv').to_numpy()\n",
    "y = pd.read_csv('labels_A2.csv').to_numpy().flatten()\n",
    "# Fill the missing value with -1\n",
    "X = np.nan_to_num(X, copy=True, nan=-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of learning methods won't behave well in the data set due to curse of dimensionality (100 features!). Thus it is important that before a model is trained, a feature selection technique is applied. \n",
    "\n",
    "We can \n",
    "1. calculate the correlation between each feature and the class label\n",
    "2. use a classifier to test out the different combianations of features (random forest will be a good option as it can randomly impute the variables in the OOB cases, and then compare them to those not in the OOB)\n",
    "3. rely on a dedicated feature selection method (PCR or Relief)\n",
    "\n",
    "In our case, we opt for the dedicated feature selection method RelieF. This is because we don't want to assume conditional independence upon the class lable betweeen features (rule out method number one) and any classifier won't function properly with these many features, so no point in testing feature importance with an underperformed classifier in the first place (rule out method number two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random state to keep a reproduciable output\n",
    "RANDOM_STATE = 1234\n",
    "np.random.seed(RANDOM_STATE)\n",
    "fs = ReliefF(n_neighbors=1, n_features_to_keep=10)\n",
    "X = fs.fit_transform(X, y)\n",
    "# X_clean = X_fill[:,np.argsort(fi)[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tree-based classifiers (decistion stump, unprunned tree, prunned tree, random forest) to establish a baseline result before we play around the data set (additive noice, multiplicative noice, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                          meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Decistion Stump               3.10  0.558525  0.065985  0.526063  0.590987   \n",
      "Decistion Tree (Prunned)      2.85  0.576525  0.050196  0.544063  0.608987   \n",
      "Decistion Tree                2.70  0.575525  0.056072  0.543063  0.607987   \n",
      "Random Forest                 1.35  0.633606  0.040060  0.601144  0.666068   \n",
      "\n",
      "                         effect_size   magnitude  \n",
      "Decistion Stump                  0.0  negligible  \n",
      "Decistion Tree (Prunned)   -0.307041       small  \n",
      "Decistion Tree             -0.277644       small  \n",
      "Random Forest              -1.375519       large  \n",
      "pvalue=0.00023423524796781616\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.31625089049339294, 0.8351843953132629, 0.02381780743598938, 0.7750453352928162]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.5368611191270642\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "def TreebasedClassifiers(X, y):\n",
    "    scores = pd.DataFrame()\n",
    "\n",
    "    ds = DecisionTreeClassifier(max_depth=1, random_state= RANDOM_STATE)\n",
    "    scores[\"Decistion Stump\"] = cross_val_score(ds, X, y, cv=10)\n",
    "\n",
    "    # we set the max-depth equal to the number of features, so it will grow to the maximum depth (unprunned)\n",
    "    dt = DecisionTreeClassifier(max_depth=10, random_state= RANDOM_STATE)\n",
    "    scores[\"Decistion Tree\"] = cross_val_score(dt, X, y, cv=10)\n",
    "\n",
    "    # we set minimal_cost_complexity_pruning close enough to 0, so it will differ from decistion stumps, but slight larger than 0, so it will differ from an unpruned decistion tree (pruned)\n",
    "    pt = DecisionTreeClassifier(random_state=RANDOM_STATE, ccp_alpha=0.005)\n",
    "    scores[\"Decistion Tree (Prunned)\"] = cross_val_score(pt, X, y, cv=10)\n",
    "\n",
    "    rf = RandomForestClassifier(max_depth=10, random_state= RANDOM_STATE)\n",
    "    scores[\"Random Forest\"] = cross_val_score(rf, X, y, cv=10)\n",
    "\n",
    "    result = autorank(scores, alpha=0.05, verbose=False)\n",
    "    print(result)\n",
    "\n",
    "TreebasedClassifiers(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision stump performed understandably the worst as it only threshhold on one feature which leads to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                          meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Decistion Stump               2.95  0.548525  0.056658  0.517434  0.579616   \n",
      "Decistion Tree (Prunned)      2.95  0.550505  0.053456  0.519414  0.581596   \n",
      "Decistion Tree                2.90  0.550505  0.043612  0.519414  0.581596   \n",
      "Random Forest                 1.20  0.624626  0.051881  0.593535  0.655717   \n",
      "\n",
      "                         effect_size   magnitude  \n",
      "Decistion Stump                  0.0  negligible  \n",
      "Decistion Tree (Prunned)   -0.035944  negligible  \n",
      "Decistion Tree              -0.03916  negligible  \n",
      "Random Forest              -1.400924       large  \n",
      "pvalue=3.436984291080978e-05\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.6545984148979187, 0.23763686418533325, 0.04130295291543007, 0.27765145897865295]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.8920393834889994\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "# additive normal noise\n",
    "noise = np.random.normal(0, 0.2, np.shape(X))\n",
    "X_addictive_noice = X + np.multiply(noise, np.average(X, axis=0))\n",
    "TreebasedClassifiers(X_addictive_noice, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classifiers remain stable, as the noise can be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                          meanrank  median    mad ci_lower  ci_upper  \\\n",
      "Decistion Stump               2.95   0.490   0.01     0.45  0.505051   \n",
      "Random Forest                 2.60   0.495  0.025     0.38      0.52   \n",
      "Decistion Tree (Prunned)      2.50   0.500    0.0     0.43  0.505051   \n",
      "Decistion Tree                1.95   0.505  0.015     0.42  0.535354   \n",
      "\n",
      "                         effect_size   magnitude  \n",
      "Decistion Stump                  0.0  negligible  \n",
      "Random Forest               -0.17713  negligible  \n",
      "Decistion Tree (Prunned)   -0.953874       large  \n",
      "Decistion Tree             -0.793671      medium  \n",
      "pvalue=0.3088962111457305\n",
      "cd=1.483221853685529\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.01277504488825798, 0.05152953788638115, 3.401902404220891e-07, 0.04103650152683258]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=0.03401818284279286\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n"
     ]
    }
   ],
   "source": [
    "# multiplicative normal noise\n",
    "noise = np.random.normal(0, 0.2, np.shape(X))\n",
    "X_multiplicative_noise = np.multiply(X, noise)\n",
    "TreebasedClassifiers(X_multiplicative_noise, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classifiers are worse as the noise can't be corrected due to randomised scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                          meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Decistion Stump               3.55  0.564566  0.041128  0.541299  0.587832   \n",
      "Decistion Tree                2.80  0.577636  0.038182   0.55437  0.600903   \n",
      "Decistion Tree (Prunned)      2.15  0.594616  0.023738   0.57135  0.617882   \n",
      "Random Forest                 1.50  0.631667  0.047512    0.6084  0.654933   \n",
      "\n",
      "                         effect_size   magnitude  \n",
      "Decistion Stump                  0.0  negligible  \n",
      "Decistion Tree             -0.329383       small  \n",
      "Decistion Tree (Prunned)   -0.894942       large  \n",
      "Random Forest              -1.510099       large  \n",
      "pvalue=0.00010545055797056721\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.5788647532463074, 0.20331569015979767, 0.3019199073314667, 0.868999183177948]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.2673054416861556\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "## class noise\n",
    "mask = np.random.binomial(1, 0.05, y.shape[0])\n",
    "y_class_noise = abs(y - mask)\n",
    "TreebasedClassifiers(X, y_class_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The multiplicative noise will have a big impact on the performance, while the other kinds of noise can be coped well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5bb4b1031e474ee5c8182ed54922ea8fe559ad6350526636e2b63cebfe2a1ae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
